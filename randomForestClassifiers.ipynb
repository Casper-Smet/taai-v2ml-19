{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "Wees goed voor de bossen, niet alleen in de natuur, maar ook voor het oplossen van problemen.\n",
    "\n",
    "*Random Forests* is een van de meest versatiele machine learning algoritmes die vandaag-de-dag beschikbaar zijn. Door gebruik van hun ingebouwde 'ensembling' (samenstelling) vermogen, is het zelf nog makkelijker geworden om ze te gebruiken om een generiek model te maken (op welke data set dan ook). \n",
    "\n",
    "Echter, vaak worden Random Forest slechts als blackbox model toegpast, zonder dat de programmeurs precies weten wat ze precies aan het doen zijn. Het makkelijkste onderdeel van Machine Learning is het coden van de oplossing; begrijpen wat er gebeurt vergt echter wat meer moeite.\n",
    "\n",
    "Random Forests worden nogal eens verward met 'bagging'; we zullen hieronder het verschil proberen uit te leggen. "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Voor- en nadelen\n",
    "Random Forests zijn een uitbereiding op Decision Trees, die reeds behandeld zijn bij Computational Modelling.\n",
    "\n",
    "De voordelen van Random Forests zijn de volgenden:\n",
    "* Random Forest kunnen zowel gebruikt worden voor classificatie en regressie problemen, en doet het op beide vlakken een redelijk goed;\n",
    "* Een van de voordelen van Random Forests is de kracht die ze heeft in het  gebruik van grote data set met hoge dimensionaliteit. RF kan eenvoudig omgaan met duizenden input variabelen en identificeert zelfstandig de belangrijkste variabelen waardoor het ook meteen als Dimension Reduction Mehtod kan worden beschouwd;\n",
    "* RF heeft een effectieve methode voor het omgaan met missende data en behoudt accuraatheid zelfs als grote delen van de data niet beschikbaar zijn;\n",
    "* RF heeft methoden om fouten in de balans tussen klassen te corrigeren;\n",
    "\n",
    "Nadelen van random forests:\n",
    "* RFs zijn prima voor classificatie, maar minder geschikt voor regressie problemen, omdat het geen continue precieze voorspelling kan doen. In het geval van regressie is RF beperkt tot het bereik van de training data. Tevens is het erg gevoelig voor overfitting als de training data erg noisy is.\n",
    "* RF wordt vaak gezien als blackbox methode, met weinig controle over wat het model precies doet. Het beste is om verschillende combinaties van parameters en random seeds te proberen.\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wat is een Random Forest?\n",
    "Random Forest is een tree-gebaseerd algoritme waarbij verschillende bomen (decision trees) worden gemaakt en worden gecombineerd zodat ze een verbetering leveren voor de generalisatie van het model. De methode waarmee bomen met elkaar worden gecombineerd heet de 'ensemble' methode. Ensembling is niets anders dan het combineren van zwakke modellen (individuele bomen) tot een sterk model.\n",
    "\n",
    "Zeg, bijvoorbeeld, dat je een film wilt gaan kijken, maar je bent onzeker over de reviews van de film. Je vraagt aan 10 personen die de film hebben gekeken, wat ze ervan vonden. 8 van hen vertellen je dat \"de film fantastisch is\". Aangezien de meerderheid lovend is over de film, besluit je om toch te gaan. Dit is een voorbeeld van ensemble in het dagelijkse leven.\n",
    "\n",
    "**Trivia**: Het random forests algoritme is gemaakt in 2001 door Leo Brieman en Adele Cutler."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hoe werkt het precies? (Decision trees)\n",
    "Om te begrijpen hoe een random forest precies werkt, moeten we eerst even bespreken hoe een decision tree if werkt:\n",
    "\n",
    "![decision tree](http://blog.hackerearth.com/wp-content/uploads/2016/12/root-01.jpg)\n",
    "\n",
    "1. Een decision tree verdeelt een gegeven een data frame (n $\\times$ p) gebaseerd op regels (if-then). Deze regels verdelen de datapunten in verschillende, niet overlappende gebieden. Deze regels worden bepaald aan de hand van hun bijdrage aan de homogeenheid of puurheid van de resulterende kind-knopen van de boom (X2, X3)\n",
    "\n",
    "2. In het plaatje hierboven wordt de variabele X1 gekozen omdat deze de hoogste puurheid levert in de kind-knopen, waardoor ze de root-node wordt. Een variabele in de root-node wordt gezien als de meest belangrijke (voorspellende) variabele in de data set.\n",
    "\n",
    "3. Maar hoe bepalen we nu precies die puurheid? Of in andere woorden, hoe bepaalt de boom op welke variabele het gaat splitsen?\n",
    "\n",
    "In regressie bomen (waar de output een voorspelling op basis van het gemiddelde van de observaties in de eindknopen) wordt de split gekozen op basis van het minimaliseren van de RSS (Root Summed Square). De variabele die de grootste vermindering in RSS bereikt wordt gekozen als split. De boom splits vervolgend met een top-down greedy aanpak (recursive binary splitting). Het splitsen wordt \"greedy\" (hebberig) genoemd omdat het algoritme de beste splits zo snel mogelijk probeert uit te voeren, in plaats van deze te bewaren voor later.\n",
    "Bij classificatie bomen (waar de output wordt bepaald door te kijken naar de observaties in de eindknopen) wordt de split bepaald door een van de volgende methoden:\n",
    "* Gini Index -- een mate van knoop \"puurheid\". Hoe kleiner de Gini index, hoe puurder de knoop (hoe meer de observaties op elkaar lijken). Om te splitsen, moet de Gini index van een kindknoop lager zijn dan die van zijn ouder.\n",
    "* Entropy -- Entropy is een mate van chaos (onpuurheid). Voor een binaire klasse ($a$ of $b$) is de Entropy maximaal als $P = 0.5$; dat wil zeggen, als P$(X=a) = 0.5$ of $P(X=b) = 0.5$, oftewel, de kans dat een nieuwe observatie label $a$ heeft is 50% (idem voor label $b$). De Entropy is minimaal als deze kans 0 of 1 is.\n",
    "$$Entropy = - p(a)*log(p(a)) - p(b)*log(p(b))$$\n",
    "\n",
    "![Entropy](http://blog.hackerearth.com/wp-content/uploads/2016/12/ent.png)\n",
    "\n",
    "In een notendop, elke boom probeert om regels te cree\\\"eren zodanig dat de resulterende eindknopen zo puur mogelijk zijn. Hoe hoger de puurheid, hoe zekerder de beslissing die wordt genomen."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bouwen van een Decision Tree\n",
    "Doe het volgende:\n",
    "* Laad de Titanic Data set;\n",
    "* Bepaal (op het oog) de belangrijkste 3 variabelen waarmee je zou kunnen bepalen of iemand overleeft;\n",
    "* Implementeer de Entropy-formule van hierboven, en bepaal de Entropy van elk van deze drie variabelen."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Titanic Data set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine best classification variables for Survival"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement Entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate and show Entropy of selected variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bouw een decision tree (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "# Kiest je sklearn model ook voor dezelfde root-variabele?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vervolg\n",
    "Decision Trees hebben te leiden van hoge variantie (\"high variance\"). \"High Variance\" betekent dat het een hoge voorspellingsfout bij ongeziene data. Dit probleem kan worden opgelost door meer data te gebruiken voor het trainen. Maar als je data set beperkt is, kan je gebruik maken van resampling technieken, zoals bagging en random forest om meer data te genereren."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hoe werkt het precies? (Random forest)\n",
    "Het bouwen van meerdere decision trees resulteert in een bos (forest). Een random forest werkt als volgt:\n",
    "Eerst maakt het gebruik van het Bagging (Bootstrap Aggregating) algoritme om random samples te creeeren. Gegeven een data set D1 ($n$ rijen en $p$ kolommen) creeert het een nieuwe data set (D2) door random $n$ gevallen te samplen. Ongeveer $\\frac{1}{3}$ van de rijen van D1 worden achterwege gelaten; deze heten de Out of Bag (OOB) samples.\n",
    "\n",
    "Vervolgens wordt het model getraind op D2. De OOB samples worden gebruikt om te nauwkeurigheid van dit model te bepalen.\n",
    "Van de $p$ kolommen worden $P << p$ kolommen geselcteerd in elke knoop. De $P$ kolommen worden random gekozen. Veelal wordt voor regressie gekozen voor een $P=p/3$ en voor classificatie voor $P=sqrt(p)$.\n",
    "\n",
    "In tegenstelling tot een boom, wordt er bij een random forest geen pruning toegepast; dat wil zeggen, elke boom wordt volledig gegroeid. Bij decision trees wordt pruning gebruikt om overfitting te voorkomen. Pruning betekent dat subtrees gekozen worden die de laagste test error rate hebben. We kunnen cross validation toepassen om de test error te bepalen van subtrees.\n",
    "\n",
    "Meerdere bomen worden op deze wijze gemaakt, en het uiteindelijke model is verkregen door het nemen van een gemiddelde of majority vote van de resultaten van elke boom.\n",
    "Elke boom wordt gegroeid op een ander deel van de originele data. Aangezien de OOB gebruikt kan worden voor het bepalen van de error, is cross validatie niet nodig bij het maken van een random forest.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bouwen van een Random Forest\n",
    "Doe het volgende:\n",
    "* Implementeer het bagging algoritme om verschillende nieuwe data sets te kunnen maken van de originele data set;\n",
    "* Creeer [50]? decision trees (sklearn trees) op nieuw gegenereerde data sets;\n",
    "* Valideer je random forest (gebruik majority voting voor ensembling) op een (van te voren) apart gezette validatie set.\n",
    "* Vergelijk je implementatie/score tegenover de sklearn implementatie: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement Bagging Algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create 50 new data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train 50 decision trees on new data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement SKLearn Random Forest on Titanic Data set\n",
    "# Compare performances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bronnen\n",
    "* [1] Practical Tutorial on Random Forest and Parameter Tuning in R: https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}